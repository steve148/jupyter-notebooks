{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Databricks Stack CLI and You"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agenda\n",
    "\n",
    "1. Who is this demo for?\n",
    "2. What is the problem?\n",
    "3. What is the databricks stack cli?\n",
    "4. How does this affect me?\n",
    "5. Where can I learn more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Who is this demo for?\n",
    "\n",
    "People who are working in databricks and are interested in how we can automate deployment of code and config to databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the problem?\n",
    "\n",
    "Current deployment process triggers on merge into the `develop` (staging) or `master` (production) branches. A CircleCI job runs which updates the notebooks within that environment. The core of that script revolves around using the databricks workspace API.\n",
    "\n",
    "```yaml\n",
    "copy_notebooks_to_databricks: &copy_notebooks_to_databricks\n",
    "  run: |\n",
    "    databricks workspace import_dir notebooks/adhoc /adhoc --overwrite\n",
    "    databricks workspace import_dir notebooks/DE_REPORT /DE_Report --overwrite\n",
    "    databricks workspace import_dir notebooks/DS_Shared /DS_Shared --overwrite\n",
    "    databricks workspace import_dir notebooks/ETL /ETL --overwrite\n",
    "    databricks workspace import_dir notebooks/public_reports /public_reports --overwrite\n",
    "    databricks workspace import_dir notebooks/SAMPLES /SAMPLES --overwrite\n",
    "    databricks workspace import_dir notebooks/tools /tools --overwrite\n",
    "    databricks workspace import_dir notebooks/utils /utils --overwrite\n",
    "```\n",
    "\n",
    "The current deployment script has some downsides.\n",
    "\n",
    "* Creating a notebook outside of the list folders shown above won't be deployed to databricks. Adding a new folder is a manual step to the deployment script which may not be intuitive.\n",
    "* Current script doesn't deploy jobs. Could use the databricks jobs API to do so, but would require a bunch of work to check which jobs exist / don't exist before deploying to an environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the Databricks Stack CLI?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copying and pasting a quote from the databricks documentation.\n",
    "\n",
    "> The stack CLI provides a way to manage a stack of Databricks resources, such as jobs, notebooks, and DBFS files.\n",
    "\n",
    "In essence the stack API provides a single interface for deploying code and configuration to databricks. This makes it easier to push changes from a local machine to our staging and production databricks workspaces.\n",
    "\n",
    "I'm not good with words, so I'll demo a simple usage of the databricks stack API here. In our demo, we are going to push a notebook and job to our staging databricks workspace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First things first, let's confirm that we have databricks installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/linuxbrew/.linuxbrew/bin/databricks\n"
     ]
    }
   ],
   "source": [
    "!which databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I heavily use the python package caled `pygments` in this notebook, so I would also just installing that if you're running the cells as you follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygments in /home/linuxbrew/.linuxbrew/lib/python3.7/site-packages (2.5.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pygments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to run a super simple snippet of a notebook that I stole from databricks. The notebook reads a text file and prints the first 10 lines in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Databricks notebook source\u001b[39;49;00m\n",
      "\n",
      "filePath = \u001b[33m\"\u001b[39;49;00m\u001b[33mdbfs:/databricks-datasets/SPARK_README.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# path in Databricks File System\u001b[39;49;00m\n",
      "lines = sc.textFile(filePath) \u001b[37m# read the file into the cluster\u001b[39;49;00m\n",
      "lines.take(\u001b[34m10\u001b[39;49;00m) \u001b[37m# display first 10 lines in the file\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize simple_notebook.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The job is also pretty simple, when it runs it will spin up a cluster and run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"Super simple job that should be deleted ASAP\"\u001b[39;49;00m,\n",
      "  \u001b[94m\"new_cluster\"\u001b[39;49;00m: {\n",
      "    \u001b[94m\"autoscale\"\u001b[39;49;00m: {\n",
      "      \u001b[94m\"min_workers\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "      \u001b[94m\"max_workers\"\u001b[39;49;00m: \u001b[34m10\u001b[39;49;00m\n",
      "    },\n",
      "    \u001b[94m\"spark_version\"\u001b[39;49;00m: \u001b[33m\"6.3.x-scala2.11\"\u001b[39;49;00m,\n",
      "    \u001b[94m\"aws_attributes\"\u001b[39;49;00m: {\n",
      "      \u001b[94m\"first_on_demand\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "      \u001b[94m\"availability\"\u001b[39;49;00m: \u001b[33m\"SPOT_WITH_FALLBACK\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"zone_id\"\u001b[39;49;00m: \u001b[33m\"us-east-1a\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"spot_bid_price_percent\"\u001b[39;49;00m: \u001b[34m100\u001b[39;49;00m,\n",
      "      \u001b[94m\"ebs_volume_type\"\u001b[39;49;00m: \u001b[33m\"GENERAL_PURPOSE_SSD\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"ebs_volume_count\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "      \u001b[94m\"ebs_volume_size\"\u001b[39;49;00m: \u001b[34m100\u001b[39;49;00m\n",
      "    },\n",
      "    \u001b[94m\"node_type_id\"\u001b[39;49;00m: \u001b[33m\"m4.large\"\u001b[39;49;00m,\n",
      "    \u001b[94m\"enable_elastic_disk\"\u001b[39;49;00m: \u001b[34mtrue\u001b[39;49;00m\n",
      "  },\n",
      "  \u001b[94m\"notebook_task\"\u001b[39;49;00m: {\n",
      "    \u001b[94m\"notebook_path\"\u001b[39;49;00m: \u001b[33m\"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\"\u001b[39;49;00m\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!pygmentize job_config.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use the databricks stack API, we'll need to create a configuration file matching their API. The important things to note in this JSON is that there are two items under the `resources` key. The first item represents the notebook in databricks, linking a file locally to a path in the databricks workspace. The second item represents a job which when run will go through the simple notebook we created earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"databricks-stack-demo\"\u001b[39;49;00m,\n",
      "  \u001b[94m\"resources\"\u001b[39;49;00m: [\n",
      "    {\n",
      "      \u001b[94m\"id\"\u001b[39;49;00m: \u001b[33m\"example-workspace-notebook\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"service\"\u001b[39;49;00m: \u001b[33m\"workspace\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"properties\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"source_path\"\u001b[39;49;00m: \u001b[33m\"simple_notebook.py\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"path\"\u001b[39;49;00m: \u001b[33m\"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"object_type\"\u001b[39;49;00m: \u001b[33m\"NOTEBOOK\"\u001b[39;49;00m\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \u001b[94m\"id\"\u001b[39;49;00m: \u001b[33m\"simple-job\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"service\"\u001b[39;49;00m: \u001b[33m\"jobs\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"properties\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"Super simple job that should be deleted ASAP\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"new_cluster\"\u001b[39;49;00m: {\n",
      "          \u001b[94m\"autoscale\"\u001b[39;49;00m: {\n",
      "            \u001b[94m\"min_workers\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "            \u001b[94m\"max_workers\"\u001b[39;49;00m: \u001b[34m10\u001b[39;49;00m\n",
      "          },\n",
      "          \u001b[94m\"spark_version\"\u001b[39;49;00m: \u001b[33m\"6.3.x-scala2.11\"\u001b[39;49;00m,\n",
      "          \u001b[94m\"aws_attributes\"\u001b[39;49;00m: {\n",
      "            \u001b[94m\"first_on_demand\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "            \u001b[94m\"availability\"\u001b[39;49;00m: \u001b[33m\"SPOT_WITH_FALLBACK\"\u001b[39;49;00m,\n",
      "            \u001b[94m\"zone_id\"\u001b[39;49;00m: \u001b[33m\"us-east-1a\"\u001b[39;49;00m,\n",
      "            \u001b[94m\"spot_bid_price_percent\"\u001b[39;49;00m: \u001b[34m100\u001b[39;49;00m,\n",
      "            \u001b[94m\"ebs_volume_type\"\u001b[39;49;00m: \u001b[33m\"GENERAL_PURPOSE_SSD\"\u001b[39;49;00m,\n",
      "            \u001b[94m\"ebs_volume_count\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "            \u001b[94m\"ebs_volume_size\"\u001b[39;49;00m: \u001b[34m100\u001b[39;49;00m\n",
      "          },\n",
      "          \u001b[94m\"node_type_id\"\u001b[39;49;00m: \u001b[33m\"m4.large\"\u001b[39;49;00m,\n",
      "          \u001b[94m\"enable_elastic_disk\"\u001b[39;49;00m: \u001b[34mtrue\u001b[39;49;00m\n",
      "        },\n",
      "        \u001b[94m\"notebook_task\"\u001b[39;49;00m: {\n",
      "          \u001b[94m\"notebook_path\"\u001b[39;49;00m: \u001b[33m\"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\"\u001b[39;49;00m\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!pygmentize staging_stack.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the basic config setup, let's see what happens when we hit the deploy button and push it to staging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Deploying stack at: staging_stack.json with options: {'overwrite': False}\n",
      "################################################################################\n",
      "Validating fields in stack configuration...\n",
      "Validating fields in resource with ID \"example-workspace-notebook\"\n",
      "Validating fields in \"properties\" of workspace resource.\n",
      "Validating fields in resource with ID \"simple-job\"\n",
      "Validating fields in \"properties\" of jobs resource.\n",
      "################################################################################\n",
      "Validating fields in stack status...\n",
      "Validating fields in resource status of resource with ID \"example-workspace-notebook\"\n",
      "Validating fields in \"databricks_id\" of workspace resource status\n",
      "Validating fields in resource status of resource with ID \"simple-job\"\n",
      "Validating fields in \"databricks_id\" of jobs resource status\n",
      "################################################################################\n",
      "Deploying stack databricks-stack-demo\n",
      "################################################################################\n",
      "Deploying workspace asset \"example-workspace-notebook\" with properties \n",
      "{\n",
      "  \"source_path\": \"simple_notebook.py\",\n",
      "  \"path\": \"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\",\n",
      "  \"object_type\": \"NOTEBOOK\"\n",
      "}\n",
      "Uploading NOTEBOOK from simple_notebook.py to Databricks workspace at /Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\n",
      "Error: b'{\"error_code\":\"RESOURCE_ALREADY_EXISTS\",\"message\":\"Path (/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook) already exists.\"}'\n"
     ]
    }
   ],
   "source": [
    "!databricks --profile staging stack deploy staging_stack.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It outputs a bunch of information on the steps it took to make sure the resources listed in the stack config file gets pushed to databricks. Now we can go look at our staging workspace and see that the notebook and job exists. (Seriously, go look. I really hope it's there).\n",
    "\n",
    "Let's say I screwed up and I want the notebook to print 100 lines, not just the first 10. Making that change in the notebook is simple, and I've made that change in a separate file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[37m# Databricks notebook source\u001b[39;49;00m\n",
      "\n",
      "filePath = \u001b[33m\"\u001b[39;49;00m\u001b[33mdbfs:/databricks-datasets/SPARK_README.md\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m \u001b[37m# path in Databricks File System\u001b[39;49;00m\n",
      "lines = sc.textFile(filePath) \u001b[37m# read the file into the cluster\u001b[39;49;00m\n",
      "lines.take(\u001b[34m100\u001b[39;49;00m) \u001b[37m# display first 10 lines in the file\u001b[39;49;00m\n"
     ]
    }
   ],
   "source": [
    "!pygmentize simple_notebook_v2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uploading this change to databricks is super simple, as we just need to rerun the deploy command again with the `--override` flag. Note that I use a v2 of the config file which points the to the new notebook with the changes we want to see deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"databricks-stack-demo\"\u001b[39;49;00m,\n",
      "  \u001b[94m\"resources\"\u001b[39;49;00m: [\n",
      "    {\n",
      "      \u001b[94m\"id\"\u001b[39;49;00m: \u001b[33m\"example-workspace-notebook\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"service\"\u001b[39;49;00m: \u001b[33m\"workspace\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"properties\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"source_path\"\u001b[39;49;00m: \u001b[33m\"simple_notebook_v2.py\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"path\"\u001b[39;49;00m: \u001b[33m\"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"object_type\"\u001b[39;49;00m: \u001b[33m\"NOTEBOOK\"\u001b[39;49;00m\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \u001b[94m\"id\"\u001b[39;49;00m: \u001b[33m\"simple-job\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"service\"\u001b[39;49;00m: \u001b[33m\"jobs\"\u001b[39;49;00m,\n",
      "      \u001b[94m\"properties\"\u001b[39;49;00m: {\n",
      "        \u001b[94m\"name\"\u001b[39;49;00m: \u001b[33m\"Super simple job that should be deleted ASAP\"\u001b[39;49;00m,\n",
      "        \u001b[94m\"new_cluster\"\u001b[39;49;00m: {\n",
      "          \u001b[94m\"autoscale\"\u001b[39;49;00m: {\n",
      "            \u001b[94m\"min_workers\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "            \u001b[94m\"max_workers\"\u001b[39;49;00m: \u001b[34m10\u001b[39;49;00m\n",
      "          },\n",
      "          \u001b[94m\"spark_version\"\u001b[39;49;00m: \u001b[33m\"6.3.x-scala2.11\"\u001b[39;49;00m,\n",
      "          \u001b[94m\"aws_attributes\"\u001b[39;49;00m: {\n",
      "            \u001b[94m\"first_on_demand\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "            \u001b[94m\"availability\"\u001b[39;49;00m: \u001b[33m\"SPOT_WITH_FALLBACK\"\u001b[39;49;00m,\n",
      "            \u001b[94m\"zone_id\"\u001b[39;49;00m: \u001b[33m\"us-east-1a\"\u001b[39;49;00m,\n",
      "            \u001b[94m\"spot_bid_price_percent\"\u001b[39;49;00m: \u001b[34m100\u001b[39;49;00m,\n",
      "            \u001b[94m\"ebs_volume_type\"\u001b[39;49;00m: \u001b[33m\"GENERAL_PURPOSE_SSD\"\u001b[39;49;00m,\n",
      "            \u001b[94m\"ebs_volume_count\"\u001b[39;49;00m: \u001b[34m1\u001b[39;49;00m,\n",
      "            \u001b[94m\"ebs_volume_size\"\u001b[39;49;00m: \u001b[34m100\u001b[39;49;00m\n",
      "          },\n",
      "          \u001b[94m\"node_type_id\"\u001b[39;49;00m: \u001b[33m\"m4.large\"\u001b[39;49;00m,\n",
      "          \u001b[94m\"enable_elastic_disk\"\u001b[39;49;00m: \u001b[34mtrue\u001b[39;49;00m\n",
      "        },\n",
      "        \u001b[94m\"notebook_task\"\u001b[39;49;00m: {\n",
      "          \u001b[94m\"notebook_path\"\u001b[39;49;00m: \u001b[33m\"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\"\u001b[39;49;00m\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "!pygmentize staging_stack_v2.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################################################\n",
      "Deploying stack at: staging_stack_v2.json with options: {'overwrite': True}\n",
      "################################################################################\n",
      "Validating fields in stack configuration...\n",
      "Validating fields in resource with ID \"example-workspace-notebook\"\n",
      "Validating fields in \"properties\" of workspace resource.\n",
      "Validating fields in resource with ID \"simple-job\"\n",
      "Validating fields in \"properties\" of jobs resource.\n",
      "################################################################################\n",
      "Validating fields in stack status...\n",
      "Validating fields in resource status of resource with ID \"example-workspace-notebook\"\n",
      "Validating fields in \"databricks_id\" of workspace resource status\n",
      "Validating fields in resource status of resource with ID \"simple-job\"\n",
      "Validating fields in \"databricks_id\" of jobs resource status\n",
      "################################################################################\n",
      "Deploying stack databricks-stack-demo\n",
      "################################################################################\n",
      "Deploying workspace asset \"example-workspace-notebook\" with properties \n",
      "{\n",
      "  \"source_path\": \"simple_notebook_v2.py\",\n",
      "  \"path\": \"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\",\n",
      "  \"object_type\": \"NOTEBOOK\"\n",
      "}\n",
      "Uploading NOTEBOOK from simple_notebook_v2.py to Databricks workspace at /Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\n",
      "################################################################################\n",
      "Deploying job \"simple-job\" with properties: \n",
      "{\n",
      "  \"name\": \"Super simple job that should be deleted ASAP\",\n",
      "  \"new_cluster\": {\n",
      "    \"autoscale\": {\n",
      "      \"min_workers\": 1,\n",
      "      \"max_workers\": 10\n",
      "    },\n",
      "    \"spark_version\": \"6.3.x-scala2.11\",\n",
      "    \"aws_attributes\": {\n",
      "      \"first_on_demand\": 1,\n",
      "      \"availability\": \"SPOT_WITH_FALLBACK\",\n",
      "      \"zone_id\": \"us-east-1a\",\n",
      "      \"spot_bid_price_percent\": 100,\n",
      "      \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n",
      "      \"ebs_volume_count\": 1,\n",
      "      \"ebs_volume_size\": 100\n",
      "    },\n",
      "    \"node_type_id\": \"m4.large\",\n",
      "    \"enable_elastic_disk\": true\n",
      "  },\n",
      "  \"notebook_task\": {\n",
      "    \"notebook_path\": \"/Users/lennox.stevenson@prodigygame.com/databricks_stack_demo/simple_notebook\"\n",
      "  }\n",
      "}\n",
      "Error: StackError: Job ID 4231 in stack status could not be found in the workspace. Please remove or make necessary changes to the current stack status to resolve this inconsistency before proceeding. Aborting stack deployment ...\n"
     ]
    }
   ],
   "source": [
    "!databricks --profile staging stack deploy staging_stack_v2.json --overwrite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, I now send you on an adventure to the staging workspace to confirm that the changes made were pushed to databricks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does this affect me?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created a [Pull Request](https://github.com/SMARTeacher/prodigy-databricks/pull/74) in the [prodigy-databricks](https://github.com/SMARTeacher/prodigy-databricks) repository to implement the stack API for the project. Assuming you follow the conventions of storing notebooks in the `notebooks` folder and jobs in the `job_config` folder, those notebooks and jobs will be automatically deployed to our databricks environments upon merging the changes into the repository.\n",
    "\n",
    "For people who aren't checking in their code into the repository, then the stack API has no affect on you or how you work. As listed at the beginging of this demo, this workflow will be more important to those building out our data ingestion pipelines or applying models they've created in their exploratory analysis to production. It is in these cases where getting code review and configuration listed in code will provide value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where can I learn more?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://github.com/SMARTeacher/prodigy-databricks/pull/74 - PR for integrating the stack API into our repository.\n",
    "* https://docs.databricks.com/dev-tools/cli/stack-cli.html - documentation on the stack CLI."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter-notebooks-neXOs9cZ",
   "language": "python",
   "name": "jupyter-notebooks-nexos9cz"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
